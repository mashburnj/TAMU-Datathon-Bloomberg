{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAMU Datathon - Bloomberg Challenge (post-competition work)\n",
    "\n",
    "I wanted to revisit this project, and see it to completion.\n",
    "\n",
    "Part 1 was guessing what 5 embeddings' original news articles were about (see other notebook).\n",
    "\n",
    "Part 2 was building a general classifier for Bloomberg's embedding system.\n",
    "\n",
    "This will have two phases:\n",
    "\n",
    "A: A genre classifier for bodies of text, trained on a large set of articles. This will be used to generate genre labels for the set of ~1,000 embeddings we were originally given at the start of the contest.\n",
    "\n",
    "B: A genre classifier for Bloomberg's embeddings. This will only be trained using the ~1,000 embeddings we were given as features, and the labels generated in Part A as targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "os.chdir('./BBC_Training_data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Classifier\n",
    "\n",
    "## Phase A: Genre Classifier, Label Generation for Embedding Set\n",
    "\n",
    "First, import the training sets (texts with genre labels) and embedding data (embeddings with texts).\n",
    "\n",
    "Second, preprocess the data using TF-IDF vectors.\n",
    "\n",
    "Third, train and cross-validate a supervised learning model for label generation.\n",
    "\n",
    "Fourth, once the model is satisfactory, apply to the embedding data.\n",
    "\n",
    "Fifth, export the embedding-label pairings (so I don't have to repeat this cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the label classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    00       000  0001  000bn  000m  000s  000th  001  001and  001st  ...  \\\n",
      "0  0.0  0.020868   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
      "1  0.0  0.000000   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
      "2  0.0  0.000000   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
      "3  0.0  0.018989   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
      "4  0.0  0.000000   0.0    0.0   0.0   0.0    0.0  0.0     0.0    0.0  ...   \n",
      "\n",
      "   zooms  zooropa  zornotza  zorro  zubair  zuluaga  zurich  zutons  \\\n",
      "0    0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
      "1    0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
      "2    0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
      "3    0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
      "4    0.0      0.0       0.0    0.0     0.0      0.0     0.0     0.0   \n",
      "\n",
      "   zvonareva  zvyagintsev  \n",
      "0        0.0          0.0  \n",
      "1        0.0          0.0  \n",
      "2        0.0          0.0  \n",
      "3        0.0          0.0  \n",
      "4        0.0          0.0  \n",
      "\n",
      "[5 rows x 29421 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.81123596, 0.90786517, 0.86067416, 0.92134831, 0.88764045])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('./business/')\n",
    "\n",
    "# next, make a NP array of every txt file in the directory\n",
    "SampleList = np.array(os.listdir())\n",
    "SampleList = SampleList[ np.char.endswith(SampleList, '.txt') ] #ignore everything that's not a txt file.\n",
    "\n",
    "#make a fresh blank series.\n",
    "Business = pd.Series(dtype='str')\n",
    "\n",
    "for SampleName in SampleList:\n",
    "    with open(SampleName) as f:\n",
    "        contents = np.array(f.readlines(), dtype = 'str')\n",
    "    contents = [line.rstrip('\\n') for line in contents]\n",
    "    #contents.str.rstrip('\\n')\n",
    "    Business[SampleName] = ' '.join(contents)\n",
    "\n",
    "#print(Business.head())\n",
    "\n",
    "\n",
    "os.chdir('../')\n",
    "os.chdir('./entertainment/')\n",
    "# next, make a NP array of every txt file in the directory\n",
    "SampleList = np.array(os.listdir())\n",
    "SampleList = SampleList[ np.char.endswith(SampleList, '.txt') ] #ignore everything that's not a txt file.\n",
    "#make a fresh blank series.\n",
    "Entertainment = pd.Series(dtype='str')\n",
    "for SampleName in SampleList:\n",
    "    with open(SampleName) as f:\n",
    "        contents = np.array(f.readlines(), dtype = 'str')\n",
    "    contents = [line.rstrip('\\n') for line in contents]\n",
    "    #contents.str.rstrip('\\n')\n",
    "    Entertainment[SampleName] = ' '.join(contents)\n",
    "\n",
    "#print(Entertainment.head())\n",
    "\n",
    "\n",
    "os.chdir('../')\n",
    "os.chdir('./politics/')\n",
    "# next, make a NP array of every txt file in the directory\n",
    "SampleList = np.array(os.listdir())\n",
    "SampleList = SampleList[ np.char.endswith(SampleList, '.txt') ] #ignore everything that's not a txt file.\n",
    "#make a fresh blank series.\n",
    "Politics = pd.Series(dtype='str')\n",
    "for SampleName in SampleList:\n",
    "    with open(SampleName) as f:\n",
    "        contents = np.array(f.readlines(), dtype = 'str')\n",
    "    contents = [line.rstrip('\\n') for line in contents]\n",
    "    #contents.str.rstrip('\\n')\n",
    "    Politics[SampleName] = ' '.join(contents)\n",
    "\n",
    "#print(Politics.head())\n",
    "\n",
    "\n",
    "os.chdir('../')\n",
    "os.chdir('./sport/')\n",
    "# next, make a NP array of every txt file in the directory\n",
    "SampleList = np.array(os.listdir())\n",
    "SampleList = SampleList[ np.char.endswith(SampleList, '.txt') ] #ignore everything that's not a txt file.\n",
    "#make a fresh blank series.\n",
    "Sports = pd.Series(dtype='str')\n",
    "for SampleName in SampleList:\n",
    "    with open(SampleName) as f:\n",
    "        contents = np.array(f.readlines(), dtype = 'str')\n",
    "    contents = [line.rstrip('\\n') for line in contents]\n",
    "    #contents.str.rstrip('\\n')\n",
    "    Sports[SampleName] = ' '.join(contents)\n",
    "\n",
    "#print(Sports.head())\n",
    "\n",
    "\n",
    "os.chdir('../')\n",
    "os.chdir('./tech/')\n",
    "# next, make a NP array of every txt file in the directory\n",
    "SampleList = np.array(os.listdir())\n",
    "SampleList = SampleList[ np.char.endswith(SampleList, '.txt') ] #ignore everything that's not a txt file.\n",
    "#make a fresh blank series.\n",
    "Technology = pd.Series(dtype='str')\n",
    "for SampleName in SampleList:\n",
    "    with open(SampleName) as f:\n",
    "        contents = np.array(f.readlines(), dtype = 'str')\n",
    "    contents = [line.rstrip('\\n') for line in contents]\n",
    "    #contents.str.rstrip('\\n')\n",
    "    Technology[SampleName] = ' '.join(contents)\n",
    "\n",
    "#print(Technology.head())\n",
    "\n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "X = vec.fit_transform(pd.concat([Business, Entertainment, Politics, Sports, Technology]))\n",
    "Features = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "\n",
    "BusLab = np.full((int(len(Business)), 1), 'Business')\n",
    "EntLab = np.full((int(len(Entertainment)), 1), 'Entertainment')\n",
    "PolLab = np.full((int(len(Politics)), 1), 'Politics')\n",
    "SpoLab = np.full((int(len(Sports)), 1), 'Sports')\n",
    "TecLab = np.full((int(len(Technology)), 1), 'Technology')\n",
    "Targets = np.concatenate([BusLab, EntLab, PolLab, SpoLab, TecLab])\n",
    "\n",
    "print(Features.head())\n",
    "\n",
    "#Let's try k-NN classification.\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "TextClassifier = KNeighborsClassifier(n_neighbors=7)\n",
    "TextClassifier.fit(Features, np.ravel(Targets))\n",
    "\n",
    "#Need to cross-validate this model.\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(TextClassifier, Features, np.ravel(Targets), cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply this model to the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File cnn_samples-54b19b96f3c0775b116bad527df8c7b5.csv does not exist: 'cnn_samples-54b19b96f3c0775b116bad527df8c7b5.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-fa372659c1bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcnn_samples0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cnn_samples-54b19b96f3c0775b116bad527df8c7b5.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Wrangling the data from strings to NP arrays.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcnn_samples1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn_samples0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'['\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m']'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File cnn_samples-54b19b96f3c0775b116bad527df8c7b5.csv does not exist: 'cnn_samples-54b19b96f3c0775b116bad527df8c7b5.csv'"
     ]
    }
   ],
   "source": [
    "cnn_samples0 = pd.read_csv('cnn_samples-54b19b96f3c0775b116bad527df8c7b5.csv')\n",
    "\n",
    "# Wrangling the data from strings to NP arrays.\n",
    "cnn_samples1 = np.fromstring((cnn_samples0.values[0,3]).replace('[','').replace(']',''), sep=',').reshape(1,512)\n",
    "\n",
    "# Rebuilding the DataFrame after this, with headline as index.\n",
    "for i in np.arange(1,np.shape(cnn_samples0)[0]):\n",
    "    temp = np.fromstring((cnn_samples0.values[i,3]).replace('[','').replace(']',''), sep=',').reshape(1,512)\n",
    "    cnn_samples1 = np.vstack([cnn_samples1, temp])\n",
    "cnn_samples = pd.DataFrame(cnn_samples1, index = cnn_samples0['text'])\n",
    "\n",
    "#Repeating the process for the challenge data.\n",
    "gov_samples0 = pd.read_csv('federal_samples-a586d0681e005629453435bea5b173eb.csv')\n",
    "gov_samples1 = np.fromstring((gov_samples0.values[0,3]).replace('[','').replace(']',''), sep=',').reshape(1,512)\n",
    "for i in np.arange(1,np.shape(gov_samples0)[0]):\n",
    "    temp = np.fromstring((gov_samples0.values[i,3]).replace('[','').replace(']',''), sep=',').reshape(1,512)\n",
    "    gov_samples1 = np.vstack([gov_samples1, temp])\n",
    "gov_samples = pd.DataFrame(gov_samples1, index = gov_samples0['text'])\n",
    "\n",
    "#also need to merge the two DataFrames\n",
    "cnngov_samples = pd.concat([cnn_samples, gov_samples], axis = 'rows')\n",
    "\n",
    "#Repeating the process for the challenge data.\n",
    "challenge0 = pd.read_csv('challenge-ddec63cf66ea88f128e3c21e457f393a.csv')\n",
    "challenge1 = np.fromstring((challenge0.values[0,1]).replace('[','').replace(']',''), sep=',').reshape(1,512)\n",
    "for i in np.arange(1,np.shape(challenge0)[0]):\n",
    "    temp = np.fromstring((challenge0.values[i,1]).replace('[','').replace(']',''), sep=',').reshape(1,512)\n",
    "    challenge1 = np.vstack([challenge1, temp])\n",
    "challenge = pd.DataFrame(challenge1, index = challenge0['id'])\n",
    "\n",
    "\n",
    "#Finally, getting the mystery 6th article.\n",
    "with open('mystery.json') as file:\n",
    "    mystery0 = json.load(file)['embedding']\n",
    "\n",
    "mystery = pd.DataFrame(np.array(mystery0).reshape(1,512), index = ['mystery'], columns = np.arange(0,512)) #it's a dict\n",
    "\n",
    "#also need to merge these two DataFrames\n",
    "challenge = pd.concat([challenge, mystery], axis = 'rows')\n",
    "\n",
    "#print(cnngov_samples.head()) #just double-checking\n",
    "#print(challenge.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase B: Genre Classifier for the Embeddings\n",
    "\n",
    "First, import the CSV file generated by the previous cell.\n",
    "\n",
    "Second, train and cross-validate a supervised learning model for label generation.\n",
    "\n",
    "Third, once the model is satisfactory, predict the genres of the five challenge articles to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
