{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAMU Datathon - Bloomberg Challenge (post-competition work)\n",
    "\n",
    "I wanted to revisit this project, and see it to completion.\n",
    "\n",
    "Part 1 was guessing what 5 embeddings' original news articles were about (see other notebook).\n",
    "\n",
    "Part 2 was building a general classifier for Bloomberg's embedding system.\n",
    "\n",
    "This will have two phases:\n",
    "\n",
    "A: A genre classifier for bodies of text, trained on a large set of articles. This will be used to generate genre labels for the set of ~1,000 embeddings we were originally given at the start of the contest.\n",
    "\n",
    "B: A genre classifier for Bloomberg's embeddings. This will only be trained using the ~1,000 embeddings we were given as features, and the labels generated in Part A as targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Classifier\n",
    "\n",
    "## Phase A: Genre Classifier, Label Generation for Embedding Set\n",
    "\n",
    "First, import the training sets (texts with genre labels) and embedding data (embeddings with texts).\n",
    "\n",
    "Second, preprocess the data using TF-IDF vectors.\n",
    "\n",
    "Third, train and cross-validate a supervised learning model for label generation.\n",
    "\n",
    "Fourth, once the model is satisfactory, apply to the embedding data.\n",
    "\n",
    "Fifth, export the embedding-label pairings (so I don't have to repeat this cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_samples0 = pd.read_csv('cnn_samples-54b19b96f3c0775b116bad527df8c7b5.csv')\n",
    "\n",
    "# Wrangling the data from strings to NP arrays.\n",
    "cnn_samples1 = np.fromstring((cnn_samples0.values[0,3]).replace('[','').replace(']',''), sep=',').reshape(1,512)\n",
    "\n",
    "# Rebuilding the DataFrame after this, with headline as index.\n",
    "for i in np.arange(1,np.shape(cnn_samples0)[0]):\n",
    "    temp = np.fromstring((cnn_samples0.values[i,3]).replace('[','').replace(']',''), sep=',').reshape(1,512)\n",
    "    cnn_samples1 = np.vstack([cnn_samples1, temp])\n",
    "cnn_samples = pd.DataFrame(cnn_samples1, index = cnn_samples0['text'])\n",
    "\n",
    "#Repeating the process for the challenge data.\n",
    "gov_samples0 = pd.read_csv('federal_samples-a586d0681e005629453435bea5b173eb.csv')\n",
    "gov_samples1 = np.fromstring((gov_samples0.values[0,3]).replace('[','').replace(']',''), sep=',').reshape(1,512)\n",
    "for i in np.arange(1,np.shape(gov_samples0)[0]):\n",
    "    temp = np.fromstring((gov_samples0.values[i,3]).replace('[','').replace(']',''), sep=',').reshape(1,512)\n",
    "    gov_samples1 = np.vstack([gov_samples1, temp])\n",
    "gov_samples = pd.DataFrame(gov_samples1, index = gov_samples0['text'])\n",
    "\n",
    "#also need to merge the two DataFrames\n",
    "cnngov_samples = pd.concat([cnn_samples, gov_samples], axis = 'rows')\n",
    "\n",
    "#Repeating the process for the challenge data.\n",
    "challenge0 = pd.read_csv('challenge-ddec63cf66ea88f128e3c21e457f393a.csv')\n",
    "challenge1 = np.fromstring((challenge0.values[0,1]).replace('[','').replace(']',''), sep=',').reshape(1,512)\n",
    "for i in np.arange(1,np.shape(challenge0)[0]):\n",
    "    temp = np.fromstring((challenge0.values[i,1]).replace('[','').replace(']',''), sep=',').reshape(1,512)\n",
    "    challenge1 = np.vstack([challenge1, temp])\n",
    "challenge = pd.DataFrame(challenge1, index = challenge0['id'])\n",
    "\n",
    "\n",
    "#Finally, getting the mystery 6th article.\n",
    "with open('mystery.json') as file:\n",
    "    mystery0 = json.load(file)['embedding']\n",
    "\n",
    "mystery = pd.DataFrame(np.array(mystery0).reshape(1,512), index = ['mystery'], columns = np.arange(0,512)) #it's a dict\n",
    "\n",
    "#also need to merge these two DataFrames\n",
    "challenge = pd.concat([challenge, mystery], axis = 'rows')\n",
    "\n",
    "#print(cnngov_samples.head()) #just double-checking\n",
    "#print(challenge.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase B: Genre Classifier for the Embeddings\n",
    "\n",
    "First, import the CSV file generated by the previous cell.\n",
    "\n",
    "Second, train and cross-validate a supervised learning model for label generation.\n",
    "\n",
    "Third, once the model is satisfactory, predict the genres of the five challenge articles to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
